<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>itwinai PyTorch Modules &mdash; itwinai 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=d45e8c67"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Integrated Use Cases" href="use_cases.html" />
    <link rel="prev" title="itwinai Tensorflow Modules" href="itwinai.tf.modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            itwinai
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">üí° Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started_with_itwinai.html">Getting started with itwinai</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ü™Ñ itwinai Modules</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">itwinai</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="itwinai.cli.html">itwinai.cli</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.cluster.html">itwinai.cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.components.html">itwinai.components</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.loggers.html">itwinai.loggers</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.parser.html">itwinai.parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.pipeline.html">itwinai.pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.serialization.html">itwinai.serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.types.html">itwinai.types</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.utils.html">itwinai.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="itwinai.tf.modules.html">itwinai Tensorflow Modules</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">itwinai PyTorch Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cluster-py">cluster.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-py">inference.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mlflow-py">mlflow.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#trainer-py">trainer.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#types-py">types.py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#utils-py">utils.py</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üìö Integrated Use-cases</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use_cases.html">Integrated Use Cases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">üöÄ Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">ML workflow tutorials</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">itwinai</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">itwinai</a></li>
      <li class="breadcrumb-item active">itwinai PyTorch Modules</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/itwinai.torch.modules.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="itwinai-pytorch-modules">
<h1>itwinai PyTorch Modules<a class="headerlink" href="#itwinai-pytorch-modules" title="Link to this heading">ÔÉÅ</a></h1>
<section id="cluster-py">
<h2>cluster.py<a class="headerlink" href="#cluster-py" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Cluster environments where to run AI workflows. Partially adapted from:</span>
<span class="sd">https://github.com/facebookresearch/detr/blob/master/util/misc.py and</span>
<span class="sd">https://github.com/ramyamounir/Template/blob/main/lib/utils/distributed.py</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>

<span class="kn">from</span> <span class="nn">..cluster</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ClusterEnvironment</span><span class="p">,</span>
    <span class="n">setup_for_distributed</span><span class="p">,</span>
    <span class="n">handle_sigusr1</span><span class="p">,</span>
    <span class="n">handle_sigterm</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.types</span> <span class="kn">import</span> <span class="n">TorchDistributedBackend</span> <span class="k">as</span> <span class="n">BackendT</span>


<span class="k">def</span> <span class="nf">fix_random_seeds</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">31</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fix random seeds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TorchCluster</span><span class="p">(</span><span class="n">ClusterEnvironment</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_set_backend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">BackendT</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unrecognized &#39;backend&#39; field. Allowed values &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;are: </span><span class="si">{</span><span class="n">BackendT</span><span class="o">.</span><span class="n">list</span><span class="p">()</span><span class="si">}</span><span class="s2">. Received &#39;</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="n">backend_name</span>

    <span class="k">def</span> <span class="nf">is_cuda_available</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">is_main_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks if the current process is the main/master process</span>
<span class="sd">        in the whole job.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">cleanup_resources</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">LocalCluster</span><span class="p">(</span><span class="n">TorchCluster</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple single node cluster with optional access to multiple GPUs.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
        <span class="n">port</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">49153</span><span class="p">,</span>
        <span class="n">rnd_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">42</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize local cluster for multi-GPU access.</span>

<span class="sd">        Args:</span>
<span class="sd">            backend (Optional[str], optional): supported PyTorch backends.</span>
<span class="sd">                If None, workload is not distributed. Defaults to None.</span>
<span class="sd">            gpus (Optional[str], optional): list of visible GPU devices</span>
<span class="sd">                (e.g., &#39;1,2,3&#39;). If empty string uses all available GPUs.</span>
<span class="sd">                If None, CPU is used. Defaults to &#39;&#39;.</span>
<span class="sd">            port (int, optional): TCP port used by the master process.</span>
<span class="sd">                Defaults to 49153.</span>
<span class="sd">            rnd_seed (Optional[int], optional): random seed to be setup after</span>
<span class="sd">                all processes are setup. Defaults to 42.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">gpus</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">port</span> <span class="o">=</span> <span class="n">port</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;tcp://127.0.0.1:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">port</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnd_seed</span> <span class="o">=</span> <span class="n">rnd_seed</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpus</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Restrict the number of GPUs visible according to user needs</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpus</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span><span class="si">}</span><span class="s2"> GPUs are available.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distributed</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># This flag tells whether the user wants to use the GPU(s)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># GPU is not manually disabled</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span>  <span class="c1"># At least one GPU is selected</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distributed has been disabled.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distributed</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CUDA disabled... Running on single CPU.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distributed</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Since single node case</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">init_dist_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">+=</span> <span class="n">worker_id</span>
            <span class="c1"># print(f&#39;GLOBAL RANK: {self.global_rank}&#39;)</span>
            <span class="c1"># Since single node case</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span>
            <span class="c1"># Simplification: worker ID mapped to GPU ID</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gpu_id</span> <span class="o">=</span> <span class="n">worker_id</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
                    <span class="n">backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
                    <span class="n">init_method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist_url</span><span class="p">,</span>
                    <span class="n">world_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span><span class="p">,</span>
                    <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span>
                <span class="p">)</span>
                <span class="n">fix_random_seeds</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnd_seed</span><span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gpu_id</span><span class="p">)</span>
                <span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

                <span class="n">setup_for_distributed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">())</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SETUP DISTRIBUTED COMPLETE&quot;</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">)</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cleanup_resources</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Distributed is disabled</span>
            <span class="c1"># Since single node case</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SLURMCluster</span><span class="p">(</span><span class="n">TorchCluster</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SLURM cluster with access to multi-node multi-GPU.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">port</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">49153</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;gloo&#39;</span><span class="p">,</span>
            <span class="n">rnd_seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">42</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">port</span> <span class="o">=</span> <span class="n">port</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnd_seed</span> <span class="o">=</span> <span class="n">rnd_seed</span>
        <span class="k">if</span> <span class="s1">&#39;SLURM_JOB_ID&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;SLURM_JOB_ID&#39; environment variable is not set. &quot;</span>
                <span class="s2">&quot;Perhaps you are not running in a slurm cluster?&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

        <span class="c1"># requeue job on SLURM preemption</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGUSR1</span><span class="p">,</span> <span class="n">handle_sigusr1</span><span class="p">)</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">handle_sigterm</span><span class="p">)</span>

        <span class="c1"># find a common host name on all nodes</span>
        <span class="n">cmd</span> <span class="o">=</span> <span class="s1">&#39;scontrol show hostnames &#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;SLURM_JOB_NODELIST&#39;</span><span class="p">)</span>
        <span class="n">stdout</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
        <span class="n">host_name</span> <span class="o">=</span> <span class="n">stdout</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;tcp://</span><span class="si">{</span><span class="n">host_name</span><span class="si">}</span><span class="s1">:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">port</span><span class="si">}</span><span class="s1">&#39;</span>

        <span class="c1"># distributed parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;SLURM_NODEID&#39;</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;SLURM_NNODES&#39;</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ngpus_per_node</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">init_dist_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">submitit</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">job_env</span> <span class="o">=</span> <span class="n">submitit</span><span class="o">.</span><span class="n">JobEnvironment</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;%j&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">job_env</span><span class="o">.</span><span class="n">job_id</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span> <span class="o">=</span> <span class="n">job_env</span><span class="o">.</span><span class="n">local_rank</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="n">job_env</span><span class="o">.</span><span class="n">global_rank</span>

            <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
                <span class="n">backend</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
                <span class="n">init_method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist_url</span><span class="p">,</span>
                <span class="n">world_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_world_size</span><span class="p">,</span>
                <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span>
            <span class="p">)</span>
            <span class="n">fix_random_seeds</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnd_seed</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
            <span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

            <span class="n">setup_for_distributed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">())</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cleanup_resources</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="inference-py">
<h2>inference.py<a class="headerlink" href="#inference-py" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">abc</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">dynamically_import_class</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">clear_key</span>
<span class="kn">from</span> <span class="nn">..components</span> <span class="kn">import</span> <span class="n">Predictor</span><span class="p">,</span> <span class="n">monitor_exec</span>
<span class="kn">from</span> <span class="nn">.types</span> <span class="kn">import</span> <span class="n">TorchDistributedStrategy</span> <span class="k">as</span> <span class="n">StrategyT</span>
<span class="kn">from</span> <span class="nn">.types</span> <span class="kn">import</span> <span class="n">Metric</span><span class="p">,</span> <span class="n">Batch</span>
<span class="kn">from</span> <span class="nn">..serialization</span> <span class="kn">import</span> <span class="n">ModelLoader</span>


<span class="k">class</span> <span class="nc">TorchModelLoader</span><span class="p">(</span><span class="n">ModelLoader</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads a torch model from somewhere.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_uri (str): Can be a path on local filesystem</span>
<span class="sd">            or an mlflow &#39;locator&#39; in the form:</span>
<span class="sd">            &#39;mlflow+MLFLOW_TRACKING_URI+RUN_ID+ARTIFACT_PATH&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Loads model from model URI.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the model URI is not recognized</span>
<span class="sd">                or the model is not found.</span>

<span class="sd">        Returns:</span>
<span class="sd">            nn.Module: torch neural network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_uri</span><span class="p">):</span>
            <span class="c1"># Model is on local filesystem.</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_uri</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;mlflow+&#39;</span><span class="p">):</span>
            <span class="c1"># Model is on an MLFLow server</span>
            <span class="c1"># Form is &#39;mlflow+MLFLOW_TRACKING_URI+RUN_ID+ARTIFACT_PATH&#39;</span>
            <span class="kn">import</span> <span class="nn">mlflow</span>
            <span class="kn">from</span> <span class="nn">mlflow</span> <span class="kn">import</span> <span class="n">MlflowException</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">tracking_uri</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="n">artifact_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_uri</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">tracking_uri</span><span class="p">)</span>

            <span class="c1"># Check that run exists</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">get_run</span><span class="p">(</span><span class="n">run_id</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">MlflowException</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run ID &#39;</span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&#39; was not found!&quot;</span><span class="p">)</span>

            <span class="c1"># Download model weights</span>
            <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">artifacts</span><span class="o">.</span><span class="n">download_artifacts</span><span class="p">(</span>
                <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
                <span class="n">artifact_path</span><span class="o">=</span><span class="n">artifact_path</span><span class="p">,</span>
                <span class="n">dst_path</span><span class="o">=</span><span class="s1">&#39;tmp/&#39;</span><span class="p">,</span>
                <span class="n">tracking_uri</span><span class="o">=</span><span class="n">mlflow</span><span class="o">.</span><span class="n">get_tracking_uri</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;Unrecognized model URI: model may not be there!&#39;</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">TorchPredictor</span><span class="p">(</span><span class="n">Predictor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies a pre-trained torch model to unseen data.&quot;&quot;&quot;</span>

    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Dataset</span>
    <span class="n">test_dataloader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_strategy</span><span class="p">:</span> <span class="n">StrategyT</span> <span class="o">=</span> <span class="n">StrategyT</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span>
    <span class="n">epoch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_glob_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">validation_glob_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span>
    <span class="n">validation_metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ModelLoader</span><span class="p">],</span>
        <span class="n">test_dataloader_class</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;torch.utils.data.DataLoader&#39;</span><span class="p">,</span>
        <span class="n">test_dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># strategy: str = StrategyT.NONE.value,</span>
        <span class="c1"># seed: Optional[int] = None,</span>
        <span class="c1"># logger: Optional[List[Logger]] = None,</span>
        <span class="c1"># cluster: Optional[ClusterEnvironment] = None,</span>
        <span class="c1"># test_metrics: Optional[Dict[str, Metric]] = None,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_parameters</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">locals2params</span><span class="p">(</span><span class="nb">locals</span><span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="c1"># self.seed = seed</span>
        <span class="c1"># self.strategy = strategy</span>
        <span class="c1"># self.cluster = cluster</span>

        <span class="c1"># Train and validation dataloaders</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_dataloader_class</span> <span class="o">=</span> <span class="n">dynamically_import_class</span><span class="p">(</span>
            <span class="n">test_dataloader_class</span>
        <span class="p">)</span>
        <span class="n">test_dataloader_kwargs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">test_dataloader_kwargs</span>
            <span class="k">if</span> <span class="n">test_dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_dataloader_kwargs</span> <span class="o">=</span> <span class="n">clear_key</span><span class="p">(</span>
            <span class="n">test_dataloader_kwargs</span><span class="p">,</span> <span class="s1">&#39;train_dataloader_kwargs&#39;</span><span class="p">,</span> <span class="s1">&#39;dataset&#39;</span>
        <span class="p">)</span>

        <span class="c1"># # Loggers</span>
        <span class="c1"># self.logger = logger if logger is not None else ConsoleLogger()</span>

        <span class="c1"># # Metrics</span>
        <span class="c1"># self.train_metrics = (</span>
        <span class="c1">#     {} if train_metrics is None else train_metrics</span>
        <span class="c1"># )</span>
        <span class="c1"># self.validation_metrics = (</span>
        <span class="c1">#     self.train_metrics if validation_metrics is None</span>
        <span class="c1">#     else validation_metrics</span>
        <span class="c1"># )</span>

    <span class="nd">@monitor_exec</span>
    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies a torch model to a dataset for inference.</span>

<span class="sd">        Args:</span>
<span class="sd">            test_dataset (Dataset[str, Any]): each item in this dataset is a</span>
<span class="sd">                couple (item_unique_id, item)</span>
<span class="sd">            model (nn.Module, optional): torch model. Overrides the existing</span>
<span class="sd">                model, if given. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: maps each item ID to the corresponding predicted</span>
<span class="sd">                value(s).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Overrides existing &quot;internal&quot; model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="n">test_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_dataloader_class</span><span class="p">(</span>
            <span class="n">test_dataset</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">test_dataloader_kwargs</span>
        <span class="p">)</span>

        <span class="n">all_predictions</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">samples_ids</span><span class="p">,</span> <span class="n">samples</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_predictions</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">pre</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">samples_ids</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
                <span class="c1"># For each item in the batch</span>
                <span class="k">if</span> <span class="n">pre</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">pre</span> <span class="o">=</span> <span class="n">pre</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pre</span> <span class="o">=</span> <span class="n">pre</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="n">all_predictions</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">pre</span>
        <span class="k">return</span> <span class="n">all_predictions</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">transform_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Batch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Post-process the predictions of the torch model (e.g., apply</span>
<span class="sd">        threshold in case of multilabel classifier).</span>
<span class="sd">        &quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">MulticlassTorchPredictor</span><span class="p">(</span><span class="n">TorchPredictor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a pre-trained torch model to unseen data for</span>
<span class="sd">    multiclass classification.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">transform_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Batch</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>


<span class="k">class</span> <span class="nc">MultilabelTorchPredictor</span><span class="p">(</span><span class="n">TorchPredictor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a pre-trained torch model to unseen data for</span>
<span class="sd">    multilabel classification, applying a threshold on the</span>
<span class="sd">    output of the neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ModelLoader</span><span class="p">],</span>
        <span class="n">test_dataloader_class</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;torch.utils.data.DataLoader&#39;</span><span class="p">,</span>
        <span class="n">test_dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">test_dataloader_class</span><span class="p">,</span> <span class="n">test_dataloader_kwargs</span><span class="p">,</span> <span class="n">name</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">transform_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Batch</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">batch</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">RegressionTorchPredictor</span><span class="p">(</span><span class="n">TorchPredictor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a pre-trained torch model to unseen data for</span>
<span class="sd">    regression, leaving untouched the output of the neural</span>
<span class="sd">    network.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">transform_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Batch</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
</section>
<section id="mlflow-py">
<h2>mlflow.py<a class="headerlink" href="#mlflow-py" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">mlflow</span>
<span class="kn">import</span> <span class="nn">yaml</span>


<span class="k">def</span> <span class="nf">_get_mlflow_logger_conf</span><span class="p">(</span><span class="n">pl_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract MLFLowLogger configuration from pytorch lightning</span>
<span class="sd">    configuration file, if present.</span>

<span class="sd">    Args:</span>
<span class="sd">        pl_config (Dict): lightning configuration loaded in memory.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optional[Dict]: if present, MLFLowLogger constructor arguments</span>
<span class="sd">        (under &#39;init_args&#39; key).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pl_config</span><span class="p">[</span><span class="s1">&#39;trainer&#39;</span><span class="p">][</span><span class="s1">&#39;logger&#39;</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c1"># If multiple loggers are provided</span>
        <span class="k">for</span> <span class="n">logger_conf</span> <span class="ow">in</span> <span class="n">pl_config</span><span class="p">[</span><span class="s1">&#39;trainer&#39;</span><span class="p">][</span><span class="s1">&#39;logger&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">logger_conf</span><span class="p">[</span><span class="s1">&#39;class_path&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;MLFlowLogger&#39;</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">logger_conf</span><span class="p">[</span><span class="s1">&#39;init_args&#39;</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">pl_config</span><span class="p">[</span><span class="s1">&#39;trainer&#39;</span><span class="p">][</span><span class="s1">&#39;logger&#39;</span><span class="p">][</span><span class="s1">&#39;class_path&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;MLFlowLogger&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">pl_config</span><span class="p">[</span><span class="s1">&#39;trainer&#39;</span><span class="p">][</span><span class="s1">&#39;logger&#39;</span><span class="p">][</span><span class="s1">&#39;init_args&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_mlflow_log_pl_config</span><span class="p">(</span><span class="n">pl_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">local_yaml_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">local_yaml_path</span><span class="p">),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">local_yaml_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">pl_config</span><span class="p">,</span> <span class="n">outfile</span><span class="p">,</span> <span class="n">default_flow_style</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">local_yaml_path</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_lightning_mlflow</span><span class="p">(</span>
    <span class="n">pl_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">default_experiment_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Default&#39;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">autolog_kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize mlflow for pytorch lightning, also setting up</span>
<span class="sd">    auto-logging (mlflow.pytorch.autolog(...)). Creates a new mlflow</span>
<span class="sd">    run and attaches it to the mlflow auto-logger.</span>

<span class="sd">    Args:</span>
<span class="sd">        pl_config (Dict): pytorch lightning configuration loaded in memory.</span>
<span class="sd">        default_experiment_name (str, optional): used as experiment name</span>
<span class="sd">        if it is not given in the lightning conf. Defaults to &#39;Default&#39;.</span>
<span class="sd">        **autolog_kwargs (kwargs): args for mlflow.pytorch.autolog(...).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mlflow_conf</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_mlflow_logger_conf</span><span class="p">(</span><span class="n">pl_config</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">mlflow_conf</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">tracking_uri</span> <span class="o">=</span> <span class="n">mlflow_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tracking_uri&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tracking_uri</span><span class="p">:</span>
        <span class="n">save_path</span> <span class="o">=</span> <span class="n">mlflow_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;save_dir&#39;</span><span class="p">)</span>
        <span class="n">tracking_uri</span> <span class="o">=</span> <span class="s2">&quot;file://&quot;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

    <span class="n">experiment_name</span> <span class="o">=</span> <span class="n">mlflow_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;experiment_name&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">experiment_name</span><span class="p">:</span>
        <span class="n">experiment_name</span> <span class="o">=</span> <span class="n">default_experiment_name</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">tracking_uri</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="n">experiment_name</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">autolog</span><span class="p">(</span><span class="o">**</span><span class="n">autolog_kwargs</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span>

    <span class="n">mlflow_conf</span><span class="p">[</span><span class="s1">&#39;experiment_name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">experiment_name</span>
    <span class="n">mlflow_conf</span><span class="p">[</span><span class="s1">&#39;run_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">active_run</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span>

    <span class="n">_mlflow_log_pl_config</span><span class="p">(</span><span class="n">pl_config</span><span class="p">,</span> <span class="s1">&#39;.tmp/pl_config.yml&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">teardown_lightning_mlflow</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;End active mlflow run, if any.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">active_run</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">end_run</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="trainer-py">
<h2>trainer.py<a class="headerlink" href="#trainer-py" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Provides training logic for PyTorch models via Trainer classes.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.distributed</span> <span class="kn">import</span> <span class="n">DistributedSampler</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="kn">from</span> <span class="nn">..components</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">monitor_exec</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">seed_worker</span><span class="p">,</span> <span class="n">par_allgather_obj</span><span class="p">,</span> <span class="n">clear_key</span>
<span class="kn">from</span> <span class="nn">.types</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Batch</span><span class="p">,</span> <span class="n">Loss</span><span class="p">,</span> <span class="n">LrScheduler</span><span class="p">,</span> <span class="n">Metric</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.types</span> <span class="kn">import</span> <span class="n">TorchDistributedStrategy</span> <span class="k">as</span> <span class="n">StrategyT</span>
<span class="kn">from</span> <span class="nn">..loggers</span> <span class="kn">import</span> <span class="n">LogMixin</span><span class="p">,</span> <span class="n">Logger</span><span class="p">,</span> <span class="n">ConsoleLogger</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">dynamically_import_class</span>
<span class="kn">from</span> <span class="nn">..cluster</span> <span class="kn">import</span> <span class="n">ClusterEnvironment</span>


<span class="k">def</span> <span class="nf">preproc_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">gwsize</span><span class="p">,</span> <span class="n">grank</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Makes a Dataloader distributed.&quot;&quot;&quot;</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
        <span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">num_replicas</span><span class="o">=</span><span class="n">gwsize</span><span class="p">,</span>
        <span class="n">rank</span><span class="o">=</span><span class="n">grank</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># Recreate dataloader, with updated sampler</span>
    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">drop_last</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">timeout</span><span class="p">,</span>
        <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>  <span class="c1"># dataloader.worker_init_fn,</span>
        <span class="n">multiprocessing_context</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">multiprocessing_context</span><span class="p">,</span>
        <span class="n">generator</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span>
        <span class="n">prefetch_factor</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">prefetch_factor</span><span class="p">,</span>
        <span class="n">persistent_workers</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">persistent_workers</span><span class="p">,</span>
        <span class="n">pin_memory_device</span><span class="o">=</span><span class="n">dataloader</span><span class="o">.</span><span class="n">pin_memory_device</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">distributed</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The decorated function must have a standard signature.</span>
<span class="sd">    Its first arguments must be:</span>
<span class="sd">    model, train_dataloader, validation_dataloader, device (in this order).</span>

<span class="sd">    Additional args or kwargs are allowed consistently with the signature</span>
<span class="sd">    of the decorated function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">dist_train</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">validation_dataloader</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">lwsize</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>  <span class="c1"># local world size - per node</span>
            <span class="n">gwsize</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>     <span class="c1"># global world size - per run</span>
            <span class="n">grank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>            <span class="c1"># global rank - assign per run</span>
            <span class="n">lrank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">%</span> <span class="n">lwsize</span>   <span class="c1"># local rank - assign per node</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gwsize</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">grank</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">lrank</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
            <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">lrank</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">lrank</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">device</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">preproc_dataloader</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">gwsize</span><span class="p">,</span> <span class="n">grank</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">validation_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validation_dataloader</span> <span class="o">=</span> <span class="n">preproc_dataloader</span><span class="p">(</span>
                <span class="n">validation_dataloader</span><span class="p">,</span> <span class="n">gwsize</span><span class="p">,</span> <span class="n">grank</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">func</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">validation_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
                 <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dist_train</span>


<span class="k">class</span> <span class="nc">TorchTrainerMG</span><span class="p">(</span><span class="n">Trainer</span><span class="p">,</span> <span class="n">LogMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Torch trainer for optionally distributed data-parallel (DDP) workload.</span>
<span class="sd">    Multi-GPU distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (nn.Module): neural network instance.</span>
<span class="sd">        loss (Loss): torch loss function instance.</span>
<span class="sd">        optimizer_class (str): path to optimizer class</span>
<span class="sd">            (e.g., &#39;torch.optim.SGD&#39;)</span>
<span class="sd">        optimizer_kwargs (Optional[Dict], optional): optimizer constructor</span>
<span class="sd">            arguments (except from parameters). Defaults to None.</span>
<span class="sd">        lr_scheduler_class (Optional[str], optional): path to learning</span>
<span class="sd">            rate scheduler class. Defaults to None.</span>
<span class="sd">        lr_scheduler_kwargs (Optional[Dict], optional): constructor arguments</span>
<span class="sd">            of the learning rate scheduler, except for the optimizer.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        train_dataloader_class (str, optional): train dataloader class path.</span>
<span class="sd">            Defaults to &#39;torch.utils.data.DataLoader&#39;.</span>
<span class="sd">        train_dataloader_kwargs (Optional[Dict], optional): constructor</span>
<span class="sd">            arguments of the train dataloader, except for the dataset</span>
<span class="sd">            instance. Defaults to None.</span>
<span class="sd">        validation_dataloader_class (str, optional): validation dataloader</span>
<span class="sd">            class path. Defaults to &#39;torch.utils.data.DataLoader&#39;.</span>
<span class="sd">        validation_dataloader_kwargs (Optional[Dict], optional): constructor</span>
<span class="sd">            arguments of the validation dataloader, except for the dataset</span>
<span class="sd">            instance. If None, it replicates `train_dataloader_kwargs`.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        epochs (int, optional): number of training epochs. Defaults to 1.</span>
<span class="sd">        strategy (Optional[TorchDistributedStrategy], optional): distributed</span>
<span class="sd">            strategy. Defaults to StrategyT.NONE.value.</span>
<span class="sd">        backend (TorchDistributedBackend, optional): computing backend.</span>
<span class="sd">            Defaults to BackendT.NCCL.value.</span>
<span class="sd">        shuffle_dataset (bool, optional): whether shuffle dataset before</span>
<span class="sd">            sampling batches from dataloader. Defaults to False.</span>
<span class="sd">        use_cuda (bool, optional): whether to use GPU. Defaults to True.</span>
<span class="sd">        benchrun (bool, optional): sets up a debug run. Defaults to False.</span>
<span class="sd">        testrun (bool, optional): deterministic training seeding everything.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        seed (Optional[int], optional): random seed. Defaults to None.</span>
<span class="sd">        logger (Optional[List[Logger]], optional): logger. Defaults to None.</span>
<span class="sd">        checkpoint_every (int, optional): how often (epochs) to checkpoint the</span>
<span class="sd">            best model. Defaults to 10.</span>
<span class="sd">        cluster (Optional[ClusterEnvironment], optional): cluster environment</span>
<span class="sd">            object describing the context in which the trainer is executed.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        train_metrics (Optional[Dict[str, Metric]], optional):</span>
<span class="sd">            list of metrics computed in the training step on the predictions.</span>
<span class="sd">            It&#39;s a dictionary with the form</span>
<span class="sd">            ``{&#39;metric_unique_name&#39;: CallableMetric}``. Defaults to None.</span>
<span class="sd">        validation_metrics (Optional[Dict[str, Metric]], optional): same</span>
<span class="sd">            as ``training_metrics``. If not given, it mirrors the training</span>
<span class="sd">            metrics. Defaults to None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: When trying to use DDP without CUDA support.</span>
<span class="sd">        NotImplementedError: when trying to use a strategy different from the</span>
<span class="sd">            ones provided by TorchDistributedStrategy.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_strategy</span><span class="p">:</span> <span class="n">StrategyT</span> <span class="o">=</span> <span class="n">StrategyT</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span>
    <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Dataset</span>
    <span class="n">validation_dataset</span><span class="p">:</span> <span class="n">Dataset</span>
    <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">validation_dataloader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">epoch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_glob_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">validation_glob_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">train_metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span>
    <span class="n">validation_metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">Loss</span><span class="p">,</span>
        <span class="n">optimizer_class</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">optimizer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lr_scheduler_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lr_scheduler_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_dataloader_class</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;torch.utils.data.DataLoader&#39;</span><span class="p">,</span>
        <span class="n">train_dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_dataloader_class</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;torch.utils.data.DataLoader&#39;</span><span class="p">,</span>
        <span class="n">validation_dataloader_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">StrategyT</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">benchrun</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">testrun</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Logger</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpoint_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">cluster</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ClusterEnvironment</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets up the distributed backend and loggers.</span>
<span class="sd">        Makes the model a DDP model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_parameters</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">locals2params</span><span class="p">(</span><span class="nb">locals</span><span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">testrun</span> <span class="o">=</span> <span class="n">testrun</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span> <span class="o">=</span> <span class="n">benchrun</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span> <span class="o">=</span> <span class="n">cluster</span>
        <span class="c1"># Checkpoint every n epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_every</span> <span class="o">=</span> <span class="n">checkpoint_every</span>

        <span class="c1"># Train and validation dataloaders</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader_class</span> <span class="o">=</span> <span class="n">dynamically_import_class</span><span class="p">(</span>
            <span class="n">train_dataloader_class</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader_class</span> <span class="o">=</span> <span class="n">dynamically_import_class</span><span class="p">(</span>
            <span class="n">validation_dataloader_class</span>
        <span class="p">)</span>
        <span class="n">train_dataloader_kwargs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">train_dataloader_kwargs</span>
            <span class="k">if</span> <span class="n">train_dataloader_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader_kwargs</span> <span class="o">=</span> <span class="n">clear_key</span><span class="p">(</span>
            <span class="n">train_dataloader_kwargs</span><span class="p">,</span> <span class="s1">&#39;train_dataloader_kwargs&#39;</span><span class="p">,</span> <span class="s1">&#39;dataset&#39;</span>
        <span class="p">)</span>
        <span class="c1"># If validation_dataloader_kwargs is not given,</span>
        <span class="c1"># copy train_dataloader_kwargs</span>
        <span class="n">validation_dataloader_kwargs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">validation_dataloader_kwargs</span> <span class="k">if</span> <span class="n">validation_dataloader_kwargs</span>
            <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">train_dataloader_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader_kwargs</span> <span class="o">=</span> <span class="n">clear_key</span><span class="p">(</span>
            <span class="n">validation_dataloader_kwargs</span><span class="p">,</span> <span class="s1">&#39;validation_dataloader_kwargs&#39;</span><span class="p">,</span>
            <span class="s1">&#39;dataset&#39;</span>
        <span class="p">)</span>

        <span class="c1"># Optimizer and scheduler</span>
        <span class="n">optim_class</span> <span class="o">=</span> <span class="n">dynamically_import_class</span><span class="p">(</span><span class="n">optimizer_class</span><span class="p">)</span>
        <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">optimizer_kwargs</span> <span class="k">if</span> <span class="n">optimizer_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="p">)</span>
        <span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="n">clear_key</span><span class="p">(</span>
            <span class="n">optimizer_kwargs</span><span class="p">,</span> <span class="s1">&#39;optimizer_kwargs&#39;</span><span class="p">,</span> <span class="s1">&#39;parameters&#39;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="n">optim_class</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_kwargs</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_scheduler_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scheduler_class</span> <span class="o">=</span> <span class="n">dynamically_import_class</span><span class="p">(</span><span class="n">lr_scheduler_class</span><span class="p">)</span>
            <span class="n">lr_scheduler_kwargs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">lr_scheduler_kwargs</span> <span class="k">if</span> <span class="n">lr_scheduler_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
            <span class="p">)</span>
            <span class="n">lr_scheduler_kwargs</span> <span class="o">=</span> <span class="n">clear_key</span><span class="p">(</span>
                <span class="n">lr_scheduler_kwargs</span><span class="p">,</span> <span class="s1">&#39;lr_scheduler_kwargs&#39;</span><span class="p">,</span> <span class="s1">&#39;optimizer&#39;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LrScheduler</span> <span class="o">=</span> <span class="n">scheduler_class</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">lr_scheduler_kwargs</span>
            <span class="p">)</span>

        <span class="c1"># Loggers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span> <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ConsoleLogger</span><span class="p">()</span>

        <span class="c1"># Metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">{}</span> <span class="k">if</span> <span class="n">train_metrics</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">train_metrics</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_metrics</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span> <span class="k">if</span> <span class="n">validation_metrics</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">validation_metrics</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">strategy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span>

    <span class="nd">@strategy</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">strategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy_name</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">strategy_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">StrategyT</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unrecognized &#39;strategy&#39; field. Allowed values &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;are: </span><span class="si">{</span><span class="n">StrategyT</span><span class="o">.</span><span class="n">list</span><span class="p">()</span><span class="si">}</span><span class="s2">. Received &#39;</span><span class="si">{</span><span class="n">strategy_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">=</span> <span class="n">strategy_name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">global_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_glob_step</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_glob_step</span>

    <span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Deterministic operations for reproducibility.</span>
<span class="sd">        Sets the random seed.</span>

<span class="sd">        Args:</span>
<span class="sd">            seed (Optional[int], optional): if not None, overrides</span>
<span class="sd">                `self.seed`. Defaults to None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">torch_rng</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">():</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="nd">@monitor_exec</span>
    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">validation_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LrScheduler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span>

        <span class="c1"># Update parameters passed for &quot;interactive&quot; use</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="k">if</span> <span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span>

        <span class="c1"># Start training</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="c1"># Make training distributed</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">ngpus_per_node</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Return value compliant with Executable.execute format</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">worker_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">):</span>
        <span class="c1"># Each worker has a different deterministic seed</span>
        <span class="c1"># Here, &#39;worker&#39; = replica of the training function</span>
        <span class="n">worker_seed</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">worker_id</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>

        <span class="c1"># Instantiate dataloaders</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_instantiate_dataloader</span><span class="p">(</span>
            <span class="n">dataloader_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader_class</span><span class="p">,</span>
            <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
            <span class="n">init_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader_kwargs</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_instantiate_dataloader</span><span class="p">(</span>
                <span class="n">dataloader_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader_class</span><span class="p">,</span>
                <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_dataset</span><span class="p">,</span>
                <span class="n">init_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader_kwargs</span>
            <span class="p">)</span>

        <span class="c1"># Launch actual training:</span>

        <span class="c1"># Single worker case</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">init_dist_gpu</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">device</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_logger</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_setup_metrics</span><span class="p">()</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">train_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="n">exc</span>
                <span class="k">finally</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;INFO: Training ended&quot;</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">destroy_logger</span><span class="p">()</span>
                    <span class="n">train_result</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">return</span> <span class="n">train_result</span>

        <span class="c1"># Init / connect to distributed backend</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">init_dist_gpu</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span> <span class="k">as</span> <span class="n">device</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_distribute_model</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup_logger</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_setup_metrics</span><span class="p">()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">train_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">exc</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;INFO: Training ended&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">destroy_logger</span><span class="p">()</span>
                <span class="n">train_result</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">train_result</span>

    <span class="k">def</span> <span class="nf">_instantiate_dataloader</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataloader_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">init_kwargs</span><span class="p">:</span> <span class="n">Dict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Make dataloader distributed if using distributed training strategy.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataloader_class (Type): some torch DataLoader type.</span>
<span class="sd">            dataset (Dataset): torch dataset instance.</span>
<span class="sd">            init_kwargs (Dict): constructor args.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init_kwargs</span><span class="p">[</span><span class="s1">&#39;generator&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;generator&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_rng</span>
        <span class="p">)</span>
        <span class="n">init_kwargs</span><span class="p">[</span><span class="s1">&#39;worker_init_fn&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;worker_init_fn&#39;</span><span class="p">,</span> <span class="n">seed_worker</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span> <span class="o">==</span> <span class="n">StrategyT</span><span class="o">.</span><span class="n">DDP</span><span class="o">.</span><span class="n">value</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                <span class="n">num_replicas</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">global_world_size</span><span class="p">,</span>
                <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                    <span class="s1">&#39;shuffle&#39;</span><span class="p">,</span> <span class="kc">False</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Overwrite existing sampler, if given.</span>
            <span class="c1"># TODO: improve using wrapper:</span>
            <span class="c1"># https://discuss.pytorch.org/t/how-to-use-my-own-sampler-when-i-already-use-distributedsampler/62143?page=2</span>
            <span class="n">init_kwargs</span><span class="p">[</span><span class="s1">&#39;sampler&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sampler</span>
            <span class="k">if</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shuffle&#39;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># sampler option is mutually exclusive with shuffle</span>
                <span class="k">del</span> <span class="n">init_kwargs</span><span class="p">[</span><span class="s1">&#39;shuffle&#39;</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">dataloader_class</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_setup_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">m_name</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">[</span><span class="n">m_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">m_name</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_metrics</span><span class="p">[</span><span class="n">m_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_distribute_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="c1"># Distribute model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span> <span class="o">==</span> <span class="n">StrategyT</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;WARNING: A GPU cluster is available but no distributed &quot;</span>
                    <span class="s2">&quot;strategy was given... Falling back to single worker...&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
                    <span class="c1"># Use only GPU:0 for single worker</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span> <span class="o">==</span> <span class="n">StrategyT</span><span class="o">.</span><span class="n">DDP</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">output_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only DDP strategy is implemented.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Trying to distribute a model when a &quot;</span>
                <span class="s2">&quot;distributed cluster is not available.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_logger</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
            <span class="c1"># Only setup loggers on main worker</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">logger</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">create_logger_context</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="n">Logger</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">create_logger_context</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized self.logger. Allowed types are &#39;list&#39; and &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;Logger&#39;. Received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">destroy_logger</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">logger</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">destroy_logger_context</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="n">Logger</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">destroy_logger_context</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized self.logger. Allowed types are &#39;list&#39; and &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;Logger&#39;. Received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">item</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
        <span class="n">identifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">kind</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;metric&#39;</span><span class="p">,</span>
        <span class="n">step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">every_worker</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">()</span> <span class="ow">or</span> <span class="n">every_worker</span><span class="p">:</span>
            <span class="c1"># Only log on main worker if not specified otherwise</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">logger</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                        <span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">,</span>
                        <span class="n">identifier</span><span class="o">=</span><span class="n">identifier</span><span class="p">,</span>
                        <span class="n">kind</span><span class="o">=</span><span class="n">kind</span><span class="p">,</span>
                        <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>
                        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="n">Logger</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">,</span>
                    <span class="n">identifier</span><span class="o">=</span><span class="n">identifier</span><span class="p">,</span>
                    <span class="n">kind</span><span class="o">=</span><span class="n">kind</span><span class="p">,</span>
                    <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>
                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized self.logger. Allowed types are &#39;list&#39; and &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;Logger&#39;. Received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Metric</span><span class="p">],</span>
        <span class="n">true</span><span class="p">:</span> <span class="n">Batch</span><span class="p">,</span>
        <span class="n">pred</span><span class="p">:</span> <span class="n">Batch</span><span class="p">,</span>
        <span class="n">logger_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;train&#39;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute and log metrics.</span>

<span class="sd">        Args:</span>
<span class="sd">            metrics (Dict[str, Metric]): metrics dict. Can be</span>
<span class="sd">                ``self.train_metrics`` or ``self.validation_metrics``.</span>
<span class="sd">            true (Batch): true values.</span>
<span class="sd">            pred (Batch): predicted values.</span>
<span class="sd">            logger_step (int): global step to pass to the logger.</span>
<span class="sd">            stage (str): &#39;train&#39;, &#39;validation&#39;...</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: metric values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m_values</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">m_name</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># metric = metric.to(self.device)</span>
            <span class="n">m_val</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">true</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">item</span><span class="o">=</span><span class="n">m_val</span><span class="p">,</span>
                <span class="n">identifier</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">m_name</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">stage</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span>
                <span class="n">step</span><span class="o">=</span><span class="n">logger_step</span><span class="p">,</span>
                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span>
            <span class="p">)</span>
            <span class="n">m_values</span><span class="p">[</span><span class="n">m_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_val</span>
        <span class="k">return</span> <span class="n">m_values</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Loss</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">Loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="n">item</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="n">identifier</span><span class="o">=</span><span class="s1">&#39;training_loss&#39;</span><span class="p">,</span>
            <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_glob_step</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span>
        <span class="p">)</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span><span class="p">(</span>
            <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">,</span>
            <span class="n">true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">pred</span><span class="o">=</span><span class="n">pred_y</span><span class="p">,</span>
            <span class="n">logger_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_glob_step</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
            <span class="n">stage</span><span class="o">=</span><span class="s1">&#39;training&#39;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Loss</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">Loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="n">item</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="n">identifier</span><span class="o">=</span><span class="s1">&#39;validation_loss&#39;</span><span class="p">,</span>
            <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_glob_step</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span>
        <span class="p">)</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_metrics</span><span class="p">(</span>
            <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_metrics</span><span class="p">,</span>
            <span class="n">true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">pred</span><span class="o">=</span><span class="n">pred_y</span><span class="p">,</span>
            <span class="n">logger_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_glob_step</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
            <span class="n">stage</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">training_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Loss</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">train_batch</span><span class="p">,</span>
                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span>
            <span class="p">)</span>
            <span class="c1"># TODO: merge and log batch metrics and loss into epoch metrics</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="c1"># Important: update counter</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_glob_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Aggregate and log losses</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">train_losses</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="n">item</span><span class="o">=</span><span class="n">avg_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="n">identifier</span><span class="o">=</span><span class="s1">&#39;training_loss_epoch&#39;</span><span class="p">,</span>
            <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_glob_step</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">avg_loss</span>

    <span class="k">def</span> <span class="nf">validation_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Loss</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">validation_losses</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">val_batch</span> \
                    <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader</span><span class="p">):</span>
                <span class="c1"># TODO: merge and log batch metrics and loss into epoch metrics</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_step</span><span class="p">(</span>
                    <span class="n">batch</span><span class="o">=</span><span class="n">val_batch</span><span class="p">,</span>
                    <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span>
                <span class="p">)</span>
                <span class="n">validation_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                <span class="c1"># Important: update counter</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">validation_glob_step</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Aggregate and log losses</span>
            <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">validation_losses</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">item</span><span class="o">=</span><span class="n">avg_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="n">identifier</span><span class="o">=</span><span class="s1">&#39;validation_loss_epoch&#39;</span><span class="p">,</span>
                <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span>
                <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_glob_step</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">avg_loss</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Undefined optimizer!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Undefined loss function!&quot;</span><span class="p">)</span>

        <span class="n">st</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Resume state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Inf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state</span><span class="p">()</span>

        <span class="c1"># start training/testing loop</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TIMER: broadcast: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">st</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;DEBUG: start training&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">56</span><span class="p">)</span>

        <span class="c1">##############################</span>
        <span class="c1"># Start training: run epochs #</span>
        <span class="c1">##############################</span>

        <span class="n">et</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">lt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1">#######################################################</span>
            <span class="c1"># Perform one training epoch and one validation epoch #</span>
            <span class="c1">#######################################################</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">:</span>
                <span class="c1"># TODO: move profiler into cluster environment</span>
                <span class="c1"># profiling (done on last epoch - slower!)</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
                    <span class="n">use_cuda</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">(),</span>
                    <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
                    <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_epoch</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_epoch</span><span class="p">()</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_epoch</span><span class="p">()</span>

            <span class="c1">#####################################</span>
            <span class="c1"># Save checkpoint if model improved #</span>
            <span class="c1">#####################################</span>

            <span class="n">ref_loss</span> <span class="o">=</span> <span class="n">val_loss</span> <span class="k">if</span> <span class="n">val_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">train_loss</span>
            <span class="n">is_best</span> <span class="o">=</span> <span class="n">ref_loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_every</span> <span class="o">==</span> <span class="mi">0</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span>
                    <span class="n">loss_val</span><span class="o">=</span><span class="n">ref_loss</span><span class="p">,</span>
                    <span class="n">is_best</span><span class="o">=</span><span class="n">is_best</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ref_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span><span class="p">)</span>

            <span class="c1">###########################</span>
            <span class="c1"># End of epoch operations #</span>
            <span class="c1">###########################</span>

            <span class="c1"># save first epoch timer</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">:</span>
                <span class="n">first_ep_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">lt</span>

            <span class="c1"># Final epoch</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">validation_dataloader</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TIMER: epoch time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">lt</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">56</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;benchmark of last epoch:&#39;</span><span class="p">)</span>
                    <span class="n">what1</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
                    <span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span>
                            <span class="n">sort_by</span><span class="o">=</span><span class="s1">&#39;self_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">what1</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;_time_total&#39;</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

        <span class="c1">##########################</span>
        <span class="c1"># Training has completed #</span>
        <span class="c1">##########################</span>

        <span class="c1"># save final state</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span>
                <span class="n">loss_val</span><span class="o">=</span><span class="n">ref_loss</span><span class="p">,</span>
                <span class="n">is_best</span><span class="o">=</span><span class="n">is_best</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="c1">########################</span>
        <span class="c1"># Print training stats #</span>
        <span class="c1">########################</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">56</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training results:&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TIMER: first epoch time: </span><span class="si">{</span><span class="n">first_ep_t</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TIMER: last epoch time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">lt</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;TIMER: average epoch time: </span><span class="si">{</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">et</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TIMER: total epoch time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">et</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;TIMER: total epoch-1 time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">et</span><span class="o">-</span><span class="n">first_ep_t</span><span class="si">}</span><span class="s1">s&#39;</span>
                <span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s1">&#39;TIMER: average epoch-1 time: &#39;</span>
                    <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">et</span><span class="o">-</span><span class="n">first_ep_t</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;TIMER: total epoch-2 time: </span><span class="si">{</span><span class="n">lt</span><span class="o">-</span><span class="n">first_ep_t</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;TIMER: average epoch-2 time: &#39;</span>
                      <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="p">(</span><span class="n">lt</span><span class="o">-</span><span class="n">first_ep_t</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
            <span class="n">mem</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span><span class="o">/</span><span class="mi">1024</span><span class="o">/</span><span class="mi">1024</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;memory req: </span><span class="si">{</span><span class="n">mem</span><span class="si">}</span><span class="s1"> MB&#39;</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">()</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span> <span class="k">else</span> <span class="s1">&#39;memory req: - MB&#39;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;memory summary:</span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TIMER: final time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">st</span><span class="si">}</span><span class="s1"> s&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">is_best</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save training state.&quot;&quot;&quot;</span>
        <span class="n">res_name</span> <span class="o">=</span> <span class="s1">&#39;checkpoint.pth.tar&#39;</span>
        <span class="n">rt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">):</span>
            <span class="c1"># find if is_best happened in any worker</span>
            <span class="n">is_best_m</span> <span class="o">=</span> <span class="n">par_allgather_obj</span><span class="p">(</span>
                <span class="n">is_best</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">global_world_size</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">is_best_m</span><span class="p">):</span>
                <span class="c1"># TODO: is this strategy really good? Checkpointing when</span>
                <span class="c1"># at least one worker improves the loss on their local</span>
                <span class="c1"># data split is prone to overfitting, especially when</span>
                <span class="c1"># the dataset in unbalanced!</span>

                <span class="c1"># find which rank is_best happened - select first rank</span>
                <span class="c1"># if multiple</span>
                <span class="n">best_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">is_best_m</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="n">best_rank</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_save_sate</span><span class="p">(</span>
                        <span class="n">epoch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">loss_val</span><span class="o">=</span><span class="n">loss_val</span><span class="p">,</span>
                        <span class="n">save_path</span><span class="o">=</span><span class="n">res_name</span>
                    <span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;DEBUG: state in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">global_rank</span><span class="si">}</span><span class="s1"> is &#39;</span>
                        <span class="sa">f</span><span class="s1">&#39;saved on epoch:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span><span class="si">}</span><span class="s1"> &#39;</span>
                        <span class="sa">f</span><span class="s1">&#39;in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">rt</span><span class="si">}</span><span class="s1"> s&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_sate</span><span class="p">(</span>
                <span class="n">epoch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">loss_val</span><span class="o">=</span><span class="n">loss_val</span><span class="p">,</span>
                <span class="n">save_path</span><span class="o">=</span><span class="n">res_name</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;DEBUG: state in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">global_rank</span><span class="si">}</span><span class="s1"> &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;is saved on epoch:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span><span class="si">}</span><span class="s1"> in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">rt</span><span class="si">}</span><span class="s1"> s&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_sate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">loss_val</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save state on disk.&quot;&quot;&quot;</span>
        <span class="n">sched</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s1">&#39;state_dict&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;best_loss&#39;</span><span class="p">:</span> <span class="n">loss_val</span><span class="p">,</span>
            <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;lr_scheduler&#39;</span><span class="p">:</span> <span class="n">sched</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="n">item</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
            <span class="n">identifier</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span>
            <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;torch&#39;</span><span class="p">,</span>
            <span class="n">epoch_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_idx</span><span class="p">,</span>
            <span class="n">batch_step</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load training state.&quot;&quot;&quot;</span>
        <span class="n">res_name</span> <span class="o">=</span> <span class="s1">&#39;checkpoint.pth.tar&#39;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">res_name</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">benchrun</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">()</span>
                        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">):</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
                    <span class="c1"># Map model to be loaded to specified single gpu.</span>
                    <span class="c1"># loc = (</span>
                <span class="c1">#     {&#39;cuda:%d&#39; % 0: &#39;cuda:%d&#39; % self.cluster.local_rank}</span>
                <span class="c1">#     if self.cluster.is_cuda_available()</span>
                <span class="c1">#     else {&#39;cpu:%d&#39; % 0: &#39;cpu:%d&#39; % self.cluster.local_rank}</span>
                    <span class="c1"># )</span>
                    <span class="c1"># checkpoint = torch.load(res_name, map_location=loc)</span>
                    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                        <span class="n">res_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">res_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;best_loss&#39;</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">])</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                        <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;lr_scheduler&#39;</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">():</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
                        <span class="nb">print</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s1">&#39;WARNING: restarting from </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="si">}</span><span class="s1"> &#39;</span>
                            <span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;WARNING: restarting from </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span><span class="si">}</span><span class="s1"> epoch&#39;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">():</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
                        <span class="nb">print</span><span class="p">(</span>
                            <span class="s1">&#39;restart file cannot be loaded, restarting!&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="s1">&#39;WARNING: restart file cannot be loaded, restarting!&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_cuda_available</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">is_main_worker</span><span class="p">():</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="s1">&#39;WARNING: given epochs are less than the &#39;</span>
                        <span class="s1">&#39;one in the restart file!&#39;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;WARNING: SYS.EXIT is issued&#39;</span><span class="p">)</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s1">&#39;WARNING: given epochs are less than the &#39;</span>
                    <span class="s1">&#39;one in the restart file!&#39;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;WARNING: SYS.EXIT is issued&#39;</span><span class="p">)</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="types-py">
<h2>types.py<a class="headerlink" href="#types-py" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Custom types definition.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">EnumMeta</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">Loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
<span class="n">LrScheduler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
<span class="n">Batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="n">Metric</span> <span class="o">=</span> <span class="n">Callable</span>


<span class="k">class</span> <span class="nc">MetaEnum</span><span class="p">(</span><span class="n">EnumMeta</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">cls</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="k">class</span> <span class="nc">BaseEnum</span><span class="p">(</span><span class="n">Enum</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">MetaEnum</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">list</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">c</span><span class="p">:</span> <span class="n">c</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">cls</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">TorchDistributedBackend</span><span class="p">(</span><span class="n">BaseEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enum for torch distributed backends.</span>
<span class="sd">    Reference: https://pytorch.org/docs/stable/distributed.html#backends</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">DEFAULT</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span>
    <span class="n">GLOO</span> <span class="o">=</span> <span class="s1">&#39;gloo&#39;</span>
    <span class="n">NCCL</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span>
    <span class="n">MPI</span> <span class="o">=</span> <span class="s1">&#39;mpi&#39;</span>


<span class="k">class</span> <span class="nc">TorchDistributedStrategy</span><span class="p">(</span><span class="n">BaseEnum</span><span class="p">):</span>
    <span class="n">DEFAULT</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">DDP</span> <span class="o">=</span> <span class="s1">&#39;ddp&#39;</span>


<span class="k">class</span> <span class="nc">TorchLoss</span><span class="p">(</span><span class="n">BaseEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Torch loss class names.</span>
<span class="sd">    TODO: complete from https://pytorch.org/docs/stable/nn.html#loss-functions</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">L1</span> <span class="o">=</span> <span class="s1">&#39;L1Loss&#39;</span>
    <span class="n">MSE</span> <span class="o">=</span> <span class="s1">&#39;MSELoss&#39;</span>
    <span class="n">CROSS_ENTROPY</span> <span class="o">=</span> <span class="s1">&#39;CrossEntropyLoss&#39;</span>
    <span class="n">NLLLOSS</span> <span class="o">=</span> <span class="s1">&#39;NLLLoss&#39;</span>


<span class="k">class</span> <span class="nc">TorchOptimizer</span><span class="p">(</span><span class="n">BaseEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Torch optimizer class names.</span>
<span class="sd">    TODO: complete from https://pytorch.org/docs/stable/optim.html#algorithms</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">SGD</span> <span class="o">=</span> <span class="s1">&#39;SGD&#39;</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="s1">&#39;Adam&#39;</span>
</pre></div>
</div>
</section>
<section id="utils-py">
<h2>utils.py<a class="headerlink" href="#utils-py" title="Link to this heading">ÔÉÅ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Hashable</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>


<span class="k">def</span> <span class="nf">save_state</span><span class="p">(</span>
    <span class="n">epoch</span><span class="p">,</span> <span class="n">distrib_model</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">res_name</span><span class="p">,</span> <span class="n">grank</span><span class="p">,</span> <span class="n">gwsize</span><span class="p">,</span>
    <span class="n">is_best</span><span class="p">,</span> <span class="n">distributed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save training state&quot;&quot;&quot;</span>
    <span class="n">rt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># find if is_best happened in any worker</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">distributed</span><span class="p">:</span>
        <span class="n">is_best_m</span> <span class="o">=</span> <span class="n">par_allgather_obj</span><span class="p">(</span><span class="n">is_best</span><span class="p">,</span> <span class="n">gwsize</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">distributed</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">is_best_m</span><span class="p">):</span>
            <span class="c1"># find which rank is_best happened - select first rank if multiple</span>
            <span class="n">is_best_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">is_best_m</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># collect state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="s1">&#39;state_dict&#39;</span><span class="p">:</span> <span class="n">distrib_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                     <span class="s1">&#39;best_loss&#39;</span><span class="p">:</span> <span class="n">loss_val</span><span class="p">,</span>
                     <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()}</span>

            <span class="c1"># write on worker with is_best</span>
            <span class="k">if</span> <span class="n">grank</span> <span class="o">==</span> <span class="n">is_best_rank</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="s1">&#39;./&#39;</span><span class="o">+</span><span class="n">res_name</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;DEBUG: state in </span><span class="si">{</span><span class="n">grank</span><span class="si">}</span><span class="s1"> is saved on &#39;</span>
                      <span class="sa">f</span><span class="s1">&#39;epoch:</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1"> in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">rt</span><span class="si">}</span><span class="s1"> s&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># collect state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="s1">&#39;state_dict&#39;</span><span class="p">:</span> <span class="n">distrib_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                 <span class="s1">&#39;best_loss&#39;</span><span class="p">:</span> <span class="n">loss_val</span><span class="p">,</span>
                 <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()}</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="s1">&#39;./&#39;</span><span class="o">+</span><span class="n">res_name</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;DEBUG: state in </span><span class="si">{</span><span class="n">grank</span><span class="si">}</span><span class="s1"> is saved on epoch:</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1"> &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">rt</span><span class="si">}</span><span class="s1"> s&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;deterministic dataloader&quot;&quot;&quot;</span>
    <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">par_allgather_obj</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">gwsize</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;gathers any object from the whole group in a list (to all workers)&quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="n">gwsize</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_object</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="c1"># print(f&#39;ALLGATHER: {res}&#39;)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">clear_key</span><span class="p">(</span>
        <span class="n">my_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
        <span class="n">dict_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">Hashable</span><span class="p">,</span>
        <span class="n">complain</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Remove key from dictionary if present and complain.</span>

<span class="sd">    Args:</span>
<span class="sd">        my_dict (Dict): Dictionary.</span>
<span class="sd">        dict_name (str): name of the dictionary.</span>
<span class="sd">        key (Hashable): Key to remove.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">my_dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">complain</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Field &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; should not be present &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;in dictionary &#39;</span><span class="si">{</span><span class="n">dict_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
            <span class="p">)</span>
        <span class="k">del</span> <span class="n">my_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">my_dict</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="itwinai.tf.modules.html" class="btn btn-neutral float-left" title="itwinai Tensorflow Modules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="use_cases.html" class="btn btn-neutral float-right" title="Integrated Use Cases" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Matteo Bunino, Alexander Zoechbauer, Kalliopi Tsolaki on behalf of CERN.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>